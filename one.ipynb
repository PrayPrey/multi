{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset.map(lambda e: tokenizer(e['en'], padding= False), batched=True)\n",
    "dataset3 = dataset.map(lambda e: tokenizer(e['de'], padding= False), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58101 # 이를 start token으로 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique([len(i) for i in dataset2['train']['input_ids']]), np.unique([len(i) for i in dataset3['train']['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    max_len = 128\n",
    "    padded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) >= max_len:\n",
    "            padded_sequence = [58101] + sequence[:max_len-1]  # 최대 길이까지 잘라냄\n",
    "        else:\n",
    "            padded_sequence = [58101] + sequence + [58100] * (max_len - len(sequence) - 1)  # 패딩 추가\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_batch = pad_sequences(dataset2['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, train_ge = torch.tensor(pad_sequences(dataset2['train']['input_ids'])), torch.tensor(pad_sequences(dataset3['train']['input_ids']))\n",
    "valid_en, valid_ge = torch.tensor(pad_sequences(dataset2['validation']['input_ids'])), torch.tensor(pad_sequences(dataset3['validation']['input_ids']))\n",
    "test_en, test_ge = torch.tensor(pad_sequences(dataset2['test']['input_ids'])), torch.tensor(pad_sequences(dataset3['test']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({128}, {128})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([len(i) for i in train_en]), set([len(i) for i in train_ge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, inputs, output):\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.output = output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.inputs[idx]\n",
    "        output = self.output[idx]\n",
    "        \n",
    "        return inputs, output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_en, train_ge)\n",
    "valid_dataset = Dataset(valid_en, valid_ge)\n",
    "test_dataset = Dataset(test_en, test_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True, drop_last = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 64, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, max_len = 128):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size + 1, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):  \n",
    "            for i in range(0, d_model, 2):  \n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        return pe\n",
    "\n",
    "    def forward(self, encoded_words):\n",
    "        \n",
    "        embedding = self.embed(encoded_words) * torch.sqrt(torch.tensor(self.d_model)).to(device)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   \n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embedding_size = 512):\n",
    "        \n",
    "#         self.data = data\n",
    "#         self.embedding_size= embedding_size\n",
    "#         self.weight_Q = nn.Linear(embedding_size, embedding_size)\n",
    "#         self.weight_K = nn.Linear(embedding_size, embedding_size)\n",
    "#         self.weight_V = nn.Linear(embedding_size, embedding_size)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        \n",
    "#     def forward(self, data):\n",
    "        \n",
    "#         Q = self.weight_Q(data)\n",
    "#         K = self.weight_K(data)\n",
    "#         V = self.weight_V(data)\n",
    "#         score = torch.matmul(Q,K.T) / torch.sqrt(self.embedding_size)\n",
    "#         value = self.softmax(score) * V\n",
    "#         return value\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inputs, outputs_input, outputs_target):\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype = torch.uint8)\n",
    "        return mask.unsqueeze(0) # 상삼각행렬 생성 -> 행과 열을 뒤 바꾸어 하삼각행렬로 바꿈. (밑에가 다 0)\n",
    "    \n",
    "    inputs_mask = inputs != 58100\n",
    "    inputs_mask = inputs_mask.to(device)\n",
    "    inputs_mask = inputs_mask.unsqueeze(1).unsqueeze(1) # 각  input에 대해서 상삼각행렬에 대응하도록 설정.\n",
    "    \n",
    "    outputs_input_mask = outputs_input != 58100\n",
    "    outputs_input_mask = outputs_input_mask.unsqueeze(1) \n",
    "    outputs_input_mask = outputs_input_mask & subsequent_mask(outputs_input.size(-1)).type_as(outputs_input_mask.data)\n",
    "    outputs_input_mask = outputs_input_mask.unsqueeze(1)\n",
    "    # masking을 해줌으로서, \n",
    "\n",
    "    \n",
    "    outputs_target_mask = outputs_target != 58100\n",
    "    \n",
    "    return inputs_mask, outputs_input_mask, outputs_target_mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key  = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        \n",
    "\n",
    "        scores = torch.matmul(query, key.permute(0 ,1 ,3, 2)) / math.sqrt(query.size(-1))\n",
    "        \n",
    "        # print(query.shape, key.shape, value.shape, mask.shape, scores.shape)\n",
    "\n",
    "        \n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # masking 된 것에 매우 작은 수 부여 -> softmax 계산시 -inf 로 계산되어짐.\n",
    "        weights = F.softmax(scores, dim = -1) # attention score 계산\n",
    "        context = torch.matmul(weights, value)  # attention value 계산\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        \n",
    "        interacted = self.concat(context)\n",
    "        \n",
    "        return interacted\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size + 1\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        encoded_layers = []\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "            encoded_layers.append(src_embeddings)\n",
    "            \n",
    "        return encoded_layers\n",
    "\n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings[i], src_mask, target_mask)\n",
    "            decoded_layers.append(tgt_embeddings)\n",
    "        return tgt_embeddings\n",
    "\n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded_layers = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded_layers, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size + 1\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "            \n",
    "        return src_embeddings\n",
    "\n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "\n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        self.lr = lr\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, prediction, target, mask):\n",
    "\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "heads = 4\n",
    "num_layers = 2\n",
    "num_layers = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "vocab_len = len(tokenizer.get_vocab())\n",
    "\n",
    "transformer = Transformer(d_model = d_model , heads = heads, num_layers = num_layers, vocab_size = vocab_len)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters())\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(vocab_len, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader.dataset))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i, (inputs, output) in enumerate(train_loader):\n",
    "        \n",
    "        samples = inputs.shape[0]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        output = output.to(device)\n",
    "        \n",
    "        output_in = output\n",
    "        output_target = output\n",
    "        \n",
    "        \n",
    "        inputs_mask, output_in_mask, output_target_mask = create_masks(inputs, output_in, output_target)\n",
    "        \n",
    "        out = transformer(inputs, inputs_mask, output_in, output_in_mask)\n",
    "        \n",
    "        loss = criterion(out, output_target, output_target_mask)\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 724.0773, 1448.1547, 1448.1547])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor((64, 128, 128)) * math.sqrt(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/453]\tLoss: 9.633\n",
      "Epoch [0][20/453]\tLoss: 9.631\n",
      "Epoch [0][40/453]\tLoss: 9.548\n",
      "Epoch [0][60/453]\tLoss: 9.424\n",
      "Epoch [0][80/453]\tLoss: 9.279\n",
      "Epoch [0][100/453]\tLoss: 9.126\n",
      "Epoch [0][120/453]\tLoss: 8.961\n",
      "Epoch [0][140/453]\tLoss: 8.783\n",
      "Epoch [0][160/453]\tLoss: 8.595\n",
      "Epoch [0][180/453]\tLoss: 8.407\n",
      "Epoch [0][200/453]\tLoss: 8.217\n",
      "Epoch [0][220/453]\tLoss: 8.027\n",
      "Epoch [0][240/453]\tLoss: 7.838\n",
      "Epoch [0][260/453]\tLoss: 7.647\n",
      "Epoch [0][280/453]\tLoss: 7.458\n",
      "Epoch [0][300/453]\tLoss: 7.268\n",
      "Epoch [0][320/453]\tLoss: 7.078\n",
      "Epoch [0][340/453]\tLoss: 6.889\n",
      "Epoch [0][360/453]\tLoss: 6.699\n",
      "Epoch [0][380/453]\tLoss: 6.513\n",
      "Epoch [0][400/453]\tLoss: 6.327\n",
      "Epoch [0][420/453]\tLoss: 6.146\n",
      "Epoch [0][440/453]\tLoss: 5.971\n",
      "Epoch [1][0/453]\tLoss: 2.064\n",
      "Epoch [1][20/453]\tLoss: 1.935\n",
      "Epoch [1][40/453]\tLoss: 1.844\n",
      "Epoch [1][60/453]\tLoss: 1.772\n",
      "Epoch [1][80/453]\tLoss: 1.701\n",
      "Epoch [1][100/453]\tLoss: 1.635\n",
      "Epoch [1][120/453]\tLoss: 1.574\n",
      "Epoch [1][140/453]\tLoss: 1.513\n",
      "Epoch [1][160/453]\tLoss: 1.460\n",
      "Epoch [1][180/453]\tLoss: 1.409\n",
      "Epoch [1][200/453]\tLoss: 1.360\n",
      "Epoch [1][220/453]\tLoss: 1.316\n",
      "Epoch [1][240/453]\tLoss: 1.274\n",
      "Epoch [1][260/453]\tLoss: 1.234\n",
      "Epoch [1][280/453]\tLoss: 1.197\n",
      "Epoch [1][300/453]\tLoss: 1.161\n",
      "Epoch [1][320/453]\tLoss: 1.127\n",
      "Epoch [1][340/453]\tLoss: 1.095\n",
      "Epoch [1][360/453]\tLoss: 1.064\n",
      "Epoch [1][380/453]\tLoss: 1.034\n",
      "Epoch [1][400/453]\tLoss: 1.006\n",
      "Epoch [1][420/453]\tLoss: 0.979\n",
      "Epoch [1][440/453]\tLoss: 0.954\n",
      "Epoch [2][0/453]\tLoss: 0.348\n",
      "Epoch [2][20/453]\tLoss: 0.384\n",
      "Epoch [2][40/453]\tLoss: 0.368\n",
      "Epoch [2][60/453]\tLoss: 0.357\n",
      "Epoch [2][80/453]\tLoss: 0.351\n",
      "Epoch [2][100/453]\tLoss: 0.342\n",
      "Epoch [2][120/453]\tLoss: 0.332\n",
      "Epoch [2][140/453]\tLoss: 0.324\n",
      "Epoch [2][160/453]\tLoss: 0.314\n",
      "Epoch [2][180/453]\tLoss: 0.307\n",
      "Epoch [2][200/453]\tLoss: 0.301\n",
      "Epoch [2][220/453]\tLoss: 0.294\n",
      "Epoch [2][240/453]\tLoss: 0.288\n",
      "Epoch [2][260/453]\tLoss: 0.282\n",
      "Epoch [2][280/453]\tLoss: 0.276\n",
      "Epoch [2][300/453]\tLoss: 0.271\n",
      "Epoch [2][320/453]\tLoss: 0.266\n",
      "Epoch [2][340/453]\tLoss: 0.261\n",
      "Epoch [2][360/453]\tLoss: 0.255\n",
      "Epoch [2][380/453]\tLoss: 0.250\n",
      "Epoch [2][400/453]\tLoss: 0.246\n",
      "Epoch [2][420/453]\tLoss: 0.241\n",
      "Epoch [2][440/453]\tLoss: 0.237\n",
      "Epoch [3][0/453]\tLoss: 0.119\n",
      "Epoch [3][20/453]\tLoss: 0.121\n",
      "Epoch [3][40/453]\tLoss: 0.122\n",
      "Epoch [3][60/453]\tLoss: 0.125\n",
      "Epoch [3][80/453]\tLoss: 0.123\n",
      "Epoch [3][100/453]\tLoss: 0.122\n",
      "Epoch [3][120/453]\tLoss: 0.120\n",
      "Epoch [3][140/453]\tLoss: 0.120\n",
      "Epoch [3][160/453]\tLoss: 0.118\n",
      "Epoch [3][180/453]\tLoss: 0.116\n",
      "Epoch [3][200/453]\tLoss: 0.115\n",
      "Epoch [3][220/453]\tLoss: 0.113\n",
      "Epoch [3][240/453]\tLoss: 0.112\n",
      "Epoch [3][260/453]\tLoss: 0.110\n",
      "Epoch [3][280/453]\tLoss: 0.109\n",
      "Epoch [3][300/453]\tLoss: 0.108\n",
      "Epoch [3][320/453]\tLoss: 0.107\n",
      "Epoch [3][340/453]\tLoss: 0.106\n",
      "Epoch [3][360/453]\tLoss: 0.104\n",
      "Epoch [3][380/453]\tLoss: 0.103\n",
      "Epoch [3][400/453]\tLoss: 0.101\n",
      "Epoch [3][420/453]\tLoss: 0.100\n",
      "Epoch [3][440/453]\tLoss: 0.099\n",
      "Epoch [4][0/453]\tLoss: 0.051\n",
      "Epoch [4][20/453]\tLoss: 0.064\n",
      "Epoch [4][40/453]\tLoss: 0.066\n",
      "Epoch [4][60/453]\tLoss: 0.063\n",
      "Epoch [4][80/453]\tLoss: 0.063\n",
      "Epoch [4][100/453]\tLoss: 0.062\n",
      "Epoch [4][120/453]\tLoss: 0.061\n",
      "Epoch [4][140/453]\tLoss: 0.061\n",
      "Epoch [4][160/453]\tLoss: 0.060\n",
      "Epoch [4][180/453]\tLoss: 0.060\n",
      "Epoch [4][200/453]\tLoss: 0.060\n",
      "Epoch [4][220/453]\tLoss: 0.059\n",
      "Epoch [4][240/453]\tLoss: 0.059\n",
      "Epoch [4][260/453]\tLoss: 0.058\n",
      "Epoch [4][280/453]\tLoss: 0.058\n",
      "Epoch [4][300/453]\tLoss: 0.057\n",
      "Epoch [4][320/453]\tLoss: 0.057\n",
      "Epoch [4][340/453]\tLoss: 0.057\n",
      "Epoch [4][360/453]\tLoss: 0.056\n",
      "Epoch [4][380/453]\tLoss: 0.056\n",
      "Epoch [4][400/453]\tLoss: 0.055\n",
      "Epoch [4][420/453]\tLoss: 0.055\n",
      "Epoch [4][440/453]\tLoss: 0.054\n",
      "Epoch [5][0/453]\tLoss: 0.048\n",
      "Epoch [5][20/453]\tLoss: 0.039\n",
      "Epoch [5][40/453]\tLoss: 0.038\n",
      "Epoch [5][60/453]\tLoss: 0.039\n",
      "Epoch [5][80/453]\tLoss: 0.039\n",
      "Epoch [5][100/453]\tLoss: 0.039\n",
      "Epoch [5][120/453]\tLoss: 0.039\n",
      "Epoch [5][140/453]\tLoss: 0.039\n",
      "Epoch [5][160/453]\tLoss: 0.038\n",
      "Epoch [5][180/453]\tLoss: 0.038\n",
      "Epoch [5][200/453]\tLoss: 0.037\n",
      "Epoch [5][220/453]\tLoss: 0.037\n",
      "Epoch [5][240/453]\tLoss: 0.037\n",
      "Epoch [5][260/453]\tLoss: 0.036\n",
      "Epoch [5][280/453]\tLoss: 0.036\n",
      "Epoch [5][300/453]\tLoss: 0.036\n",
      "Epoch [5][320/453]\tLoss: 0.036\n",
      "Epoch [5][340/453]\tLoss: 0.036\n",
      "Epoch [5][360/453]\tLoss: 0.036\n",
      "Epoch [5][380/453]\tLoss: 0.036\n",
      "Epoch [5][400/453]\tLoss: 0.035\n",
      "Epoch [5][420/453]\tLoss: 0.035\n",
      "Epoch [5][440/453]\tLoss: 0.035\n",
      "Epoch [6][0/453]\tLoss: 0.025\n",
      "Epoch [6][20/453]\tLoss: 0.025\n",
      "Epoch [6][40/453]\tLoss: 0.025\n",
      "Epoch [6][60/453]\tLoss: 0.025\n",
      "Epoch [6][80/453]\tLoss: 0.024\n",
      "Epoch [6][100/453]\tLoss: 0.024\n",
      "Epoch [6][120/453]\tLoss: 0.024\n",
      "Epoch [6][140/453]\tLoss: 0.024\n",
      "Epoch [6][160/453]\tLoss: 0.025\n",
      "Epoch [6][180/453]\tLoss: 0.025\n",
      "Epoch [6][200/453]\tLoss: 0.025\n",
      "Epoch [6][220/453]\tLoss: 0.025\n",
      "Epoch [6][240/453]\tLoss: 0.025\n",
      "Epoch [6][260/453]\tLoss: 0.025\n",
      "Epoch [6][280/453]\tLoss: 0.025\n",
      "Epoch [6][300/453]\tLoss: 0.025\n",
      "Epoch [6][320/453]\tLoss: 0.025\n",
      "Epoch [6][340/453]\tLoss: 0.025\n",
      "Epoch [6][360/453]\tLoss: 0.025\n",
      "Epoch [6][380/453]\tLoss: 0.025\n",
      "Epoch [6][400/453]\tLoss: 0.025\n",
      "Epoch [6][420/453]\tLoss: 0.025\n",
      "Epoch [6][440/453]\tLoss: 0.025\n",
      "Epoch [7][0/453]\tLoss: 0.015\n",
      "Epoch [7][20/453]\tLoss: 0.017\n",
      "Epoch [7][40/453]\tLoss: 0.018\n",
      "Epoch [7][60/453]\tLoss: 0.019\n",
      "Epoch [7][80/453]\tLoss: 0.019\n",
      "Epoch [7][100/453]\tLoss: 0.019\n",
      "Epoch [7][120/453]\tLoss: 0.019\n",
      "Epoch [7][140/453]\tLoss: 0.018\n",
      "Epoch [7][160/453]\tLoss: 0.019\n",
      "Epoch [7][180/453]\tLoss: 0.019\n",
      "Epoch [7][200/453]\tLoss: 0.018\n",
      "Epoch [7][220/453]\tLoss: 0.018\n",
      "Epoch [7][240/453]\tLoss: 0.018\n",
      "Epoch [7][260/453]\tLoss: 0.018\n",
      "Epoch [7][280/453]\tLoss: 0.018\n",
      "Epoch [7][300/453]\tLoss: 0.018\n",
      "Epoch [7][320/453]\tLoss: 0.019\n",
      "Epoch [7][340/453]\tLoss: 0.018\n",
      "Epoch [7][360/453]\tLoss: 0.018\n",
      "Epoch [7][380/453]\tLoss: 0.019\n",
      "Epoch [7][400/453]\tLoss: 0.018\n",
      "Epoch [7][420/453]\tLoss: 0.018\n",
      "Epoch [7][440/453]\tLoss: 0.018\n",
      "Epoch [8][0/453]\tLoss: 0.013\n",
      "Epoch [8][20/453]\tLoss: 0.014\n",
      "Epoch [8][40/453]\tLoss: 0.014\n",
      "Epoch [8][60/453]\tLoss: 0.014\n",
      "Epoch [8][80/453]\tLoss: 0.014\n",
      "Epoch [8][100/453]\tLoss: 0.014\n",
      "Epoch [8][120/453]\tLoss: 0.014\n",
      "Epoch [8][140/453]\tLoss: 0.014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer_optimizer}\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m transformer_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 27\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m samples\n\u001b[0;32m     28\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m samples\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    \n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "heads = 4\n",
    "num_layers = 2\n",
    "num_layers = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "vocab_len = len(tokenizer.get_vocab()) + 1\n",
    "\n",
    "\n",
    "transformer = Transformer(d_model = d_model , heads = heads, num_layers = num_layers, vocab_size = vocab_len)\n",
    "transformer = transformer.to(device)\n",
    "# adam_optimizer = torch.optim.Adam(transformer.parameters())\n",
    "# transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(vocab_len, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "# 저장된 파일을 불러올 때\n",
    "checkpoint = torch.load('checkpoint_7.pth.tar')\n",
    "\n",
    "# 불러온 checkpoint에서 모델 상태나 다른 필요한 요소들을 추출할 수 있습니다.\n",
    "transformer = deepcopy(checkpoint['transformer'])\n",
    "\n",
    "# 모델을 evaluation 모드로 설정 (필요에 따라)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = {v:k for k,v in tokenizer.get_vocab().items()} \n",
    "start_token = dict['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict['<start>'] = 58101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = dict['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, dict):\n",
    "\n",
    "    transformer.eval()\n",
    "\n",
    "    bb = {v:k for k,v in tokenizer.get_vocab().items()} \n",
    "    start_token = 58101\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "\n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions[:,:-1], dim = 1)\n",
    "        \n",
    "        next_word = next_word.item()\n",
    "  \n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "\n",
    "\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "\n",
    "    print(words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58101,    93,  2896,   564, 15202,  1729,    14, 24185,  4470,     2,\n",
       "        16209,    79,    14, 27192,    18,     7, 24185,  2729,    56,     3,\n",
       "            0, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58101, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m question \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(enc_qus)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m question_mask \u001b[38;5;241m=\u001b[39m (question\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m58100\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "Cell \u001b[1;32mIn[59], line 29\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(transformer, question, question_mask, max_len, dict)\u001b[0m\n\u001b[0;32m     25\u001b[0m     words \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msentence\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "max_len = 128\n",
    "enc_qus = [58101,    93,  2896,   564, 15202,  1729,    14, 24185,  4470,     2,\n",
    "        16209,    79,    14, 27192,    18,     7, 24185,  2729,    56,     3,\n",
    "            0, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
    "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=58100).to(device).unsqueeze(1).unsqueeze(1)\n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), dict)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feg = []\n",
    "for ww in A.detach().clone().cpu().numpy():\n",
    "    feg_small = []\n",
    "    for www in ww:\n",
    "        if www == 0:\n",
    "            break\n",
    "        else:\n",
    "            feg_small.append(bb[www])\n",
    "        \n",
    "    feg.append(feg_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feg = []\n",
    "for ww in A.detach().clone().cpu().numpy():\n",
    "    feg_small = []\n",
    "    for www in ww:\n",
    "        if www == 0:\n",
    "            break\n",
    "        else:\n",
    "            feg_small.append(bb[www])\n",
    "        \n",
    "    feg.append(feg_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.exp(out.detach().clone()),dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = [tokenizer.tokenize(i) for i in dataset3['test']['de']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i == j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "960it [00:01, 658.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "scores = []\n",
    "for i,j in tqdm(zip(feg, fff)):\n",
    "    score1 = corpus_bleu(i, j)\n",
    "    scores.append(score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
